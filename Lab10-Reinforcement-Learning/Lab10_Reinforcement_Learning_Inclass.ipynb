{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_hDqbGcEh8z"
   },
   "source": [
    "# Lab10 Introduction to Reinforcement Learning\n",
    "\n",
    "- Name, Student ID 1\n",
    "- Name, Student ID 2\n",
    "\n",
    "## Lab Instruction\n",
    "\n",
    "This lab, we will implement an Q-learning agent that will solve the GridWorld problem. The objective of your agent is to maximize the reward by find the shortest path to the exit without stepping on a bomb or a cliff. The result will be compare with the randomly move agent.\n",
    "\n",
    "###The total lab score is 10 which will be evaluated as follows:\n",
    "\n",
    "- Creative  (Do as the instruction said. This include the model tuning section where you have to do a proper amount of tuning) - 4 points\n",
    "- Design of logic (No weired things in the process) - 2 points\n",
    "- Journaling (Communicate your thought process and discuss result & analyse in every step) - 4 points\n",
    "\n",
    "### Reinforcement Learning\n",
    "\n",
    "The reinforcement learning components:</br>\n",
    "<img src='https://github.com/fongfongfong/CPE_CodeInAI_Course_2018/blob/master/9_RL/img/rl.png?raw=1' width=500>\n",
    "\n",
    "- Reinforcement Learning Blog Post (Thai): <a href=\"https://medium.com/asquarelab/ep-1-reinforcement-learning-%E0%B9%80%E0%B8%9A%E0%B8%B7%E0%B9%89%E0%B8%AD%E0%B8%87%E0%B8%95%E0%B9%89%E0%B8%99-acfa9d42394c\"> Thammasorn, A-Square</a></br>\n",
    "- Reinforcement Learning Blog Post (Eng): <a href=\"http://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/\"> Adventures in Machine Learning</a>\n",
    "\n",
    "### Temporal-Different Learning\n",
    "**Temporal-difference (TD) learning** is a combination of **Monte Carlo** ideas and **dynamic programming (DP)** ideas.</br>\n",
    "Like Monte Carlo methods, TD methods **can learn directly from raw experience without a model of the environmentâ€™s dynamics**.</br> \n",
    "Like DP, TD methods update estimates based in part on other learned estimates, **without waiting for a final outcome** (they bootstrap).</br>\n",
    "The relationship between TD, DP, and Monte Carlo methods is a recurring theme in the theory of reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_hEbeh5XcNb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ooIUU9BMEh84"
   },
   "outputs": [],
   "source": [
    "# Setup \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_yqpG1jlEh86"
   },
   "source": [
    "### Environment - Cliff Gridworld\n",
    "\n",
    "The environment is a Cliff Gridworld, illustrated as follows: </br>\n",
    "\n",
    "<img src='https://github.com/fongfongfong/CPE_CodeInAI_Course_2018/blob/master/9_RL/img/grid.png?raw=1' width=500></br>\n",
    "\n",
    "The world is a 10x10 grid where the exit is at bottom right corner. The bomb is randomly generated around the exit.</br>\n",
    "The agent is randomly start around the top left corner section. </br>\n",
    "Each step count as -1. In other word, the more step your agent take, the more penalty you have.</br>\n",
    "If your agent move against the corner, your agent will move. If your agent step to the cliff, the penalty is -20, same as stepping into a bomb which get -10 penalty.</br>\n",
    "If your agent reach to the exit, you will recieve 20 reward.</br>\n",
    "\n",
    "The episode is end when your agent found a way out and get a reward or step on a bomb and get the penalty point.</br>\n",
    "Your agent is at the starting point when the new episode begin.\n",
    "\n",
    "```env.World```</br>\n",
    ">**Properties**\n",
    "- height: The height of a grid world\n",
    "- width: The width of a grid world\n",
    "- current_location: current location of your agent.\n",
    "- actions: a list of an available actions (up,down,left,right)</br>\n",
    "\n",
    ">**Methods**\n",
    "- ```available_actions()``` Get a set of available actions\n",
    "- ```move_agent(action)``` Move an agent to the given direction and return a reward of that action.\n",
    "- ```reset()``` Reset the state of the environment to the starting point.\n",
    "- ```end_state()``` Get the end state. Return True if the state is end. False, otherwise.\n",
    "- ```render()``` Show the current terran of the grid world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vVzQ4g-PEh87"
   },
   "outputs": [],
   "source": [
    "# Initialize environment\n",
    "env = env.World()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.current_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.available_actions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ig2J3GhEh88"
   },
   "source": [
    "### Define Random Agent\n",
    "\n",
    "Define an agent that walk randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yucS1fd7RrMl"
   },
   "source": [
    "Hint : use \n",
    "` np.random.choice` to random available action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4QyYnWTzEh88"
   },
   "outputs": [],
   "source": [
    "# Define Random Agent Object\n",
    "class DumbAgent():\n",
    "    def action(self, available_actions):\n",
    "        #\n",
    "        # Code Here\n",
    "        #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VZDzMBgwEh89"
   },
   "outputs": [],
   "source": [
    "# Test your code\n",
    "agent = DumbAgent()\n",
    "agent.action([1,2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utcDiCzLEh8-"
   },
   "source": [
    "###  Define a Q-Agent\n",
    "\n",
    "Define an agent that used q-learning using the following pseudocode: </br>\n",
    "<img src='https://github.com/fongfongfong/CPE_CodeInAI_Course_2018/blob/master/9_RL/img/q_learning.png?raw=1'>\n",
    "\n",
    "- S is a state of the environment\n",
    "- S' is a next state\n",
    "- A is a action choosen by the agent\n",
    "- A' is a next action\n",
    "- epsilon, alpha, gamma are the parameter for the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FB9B7EO1Eh8_"
   },
   "outputs": [],
   "source": [
    "class Q_Agent():\n",
    "    \n",
    "    def __init__(self, environment, alpha, gamma, epsilon):\n",
    "        self.environment = environment\n",
    "        self.q_table = dict()\n",
    "        for x in range(environment.height):\n",
    "            for y in range(environment.width):\n",
    "                self.q_table[(x,y)] = {'up':0,'down':0,'left':0,'right':0}\n",
    "                \n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        \n",
    "    def action(self, available_actions):\n",
    "        if np.random.uniform(0,1) < self.epsilon:\n",
    "            # Exploration\n",
    "            return np.random.choice(available_actions)\n",
    "        else:\n",
    "            # Exploitation\n",
    "            S = self.q_table[self.environment.current_location]\n",
    "            max_value = max(S.values())\n",
    "            # random if you have multiple max value\n",
    "            return np.random.choice([key for key, value in S.items() if value == max_value])\n",
    "            \n",
    "            \n",
    "    def update(self, current_state, reward, next_state, actions):\n",
    "        S_prime = self.q_table[next_state]\n",
    "        max_value_S_prime = max(S_prime.values())\n",
    "        current_Q_value = self.q_table[current_state][actions]\n",
    "        \n",
    "        # From the equation in pseudocode\n",
    "        self.q_table[current_state][actions] = current_Q_value + self.alpha * (reward + self.gamma*max_value_S_prime - current_Q_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-y7I9hgEh9A"
   },
   "source": [
    "###  Deploy Your Agent\n",
    "\n",
    "Create a function to run your agent in an environment. Your agent will run n trails, each trails has a maximum of m max_steps_per_episode.</br>\n",
    "\n",
    "The function must record the total reward in each trials and return when the function end.\n",
    "```python\n",
    "total_reward = run(env, agent, trials, max_steps_per_episode, learn=False)\n",
    "```\n",
    "\n",
    "For the Q-agent, you have to update Q-value for your agent using following code: </br>\n",
    "```python\n",
    "if learn: \n",
    "    agent.update( ... )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wwItjL_VEh9B"
   },
   "outputs": [],
   "source": [
    "def run(env, agent, trials, max_steps_per_episode, learn=False):\n",
    "    #\n",
    "    # Code here\n",
    "    # Sent the action to environment and get the next state \n",
    "    # and reward\n",
    "    # Hint: \n",
    "    #     Define total_reward, sum_reward, step, \n",
    "    #            current_state, action, reward & next_state \n",
    "    #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GZ3K-LmHEh9B"
   },
   "outputs": [],
   "source": [
    "# Plot reward\n",
    "def plot_total_reward(total_reward):\n",
    "    plt.plot(total_reward)\n",
    "    plt.title('The Sum of Reward During Each Episode')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Cumulative Reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tY7JAZkEh9B"
   },
   "source": [
    "### Run a Dump Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ekq64BmFEh9C"
   },
   "outputs": [],
   "source": [
    "## Run\n",
    "env.reset()\n",
    "agent =  DumbAgent()\n",
    "total_reward = run(env, agent, trials=100, max_steps_per_episode=200, learn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6J3i2GoWEh9C"
   },
   "outputs": [],
   "source": [
    "## Plot reward\n",
    "plot_total_reward(total_reward )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DBSG6YjEh9C"
   },
   "source": [
    "### Run an Q-Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FFB46_2WEh9D"
   },
   "outputs": [],
   "source": [
    "## Run\n",
    "env.reset()\n",
    "agent = Q_Agent(env, alpha=0.01, gamma=1, epsilon=0.05)\n",
    "total_reward = run(env, agent, trials=100, max_steps_per_episode=200, learn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PoF05iXeEh9D"
   },
   "outputs": [],
   "source": [
    "## Plot reward\n",
    "plot_total_reward(total_reward )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbP1HeAfEh9D"
   },
   "source": [
    "### Results and Discussion\n",
    "\n",
    "Compare the result from a dump agent, as a based line, and the q-agent with different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v_d9Pbo8Eh9D"
   },
   "outputs": [],
   "source": [
    "# Write your discussion in a Markdown Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dv4LI3uNEh9E"
   },
   "source": [
    "### Show Q-Table\n",
    "\n",
    "Show the agent's Q-table using to make a decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o3-KTIitEh9E"
   },
   "outputs": [],
   "source": [
    "def Viz_q_table(d, indent=0):\n",
    "    for key, value in d.items():\n",
    "        print('\\t' * indent + str(key))\n",
    "        if isinstance(value, dict):\n",
    "            Viz_q_table(value, indent+1)\n",
    "        else:\n",
    "            print('\\t' * (indent+1) + str(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FvtvG4_3Eh9E"
   },
   "outputs": [],
   "source": [
    "Viz_q_table(agentQ.q_table)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab10_Reinforcement_Learning_Inclass.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
