{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "127757f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1b3ddde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from keras.preprocessing.image import load_img, ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.convolutional import  MaxPooling2D\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import splitfolders\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e95100ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from keras import backend as K\n",
    "\n",
    "# define a function to plot the result from training step\n",
    "def show_result(history): \n",
    "    \n",
    "    # Print the result from the last epoch\n",
    "    print('Last train accuracy: %s'%history.history['accuracy'][-1])\n",
    "    print('Last validation accuracy: %s'%history.history['val_accuracy'][-1])\n",
    "    \n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    \n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    \n",
    "    epochs = range(1, len(loss) + 1)   \n",
    "    \n",
    "    # Define a subplot \n",
    "    fig, axs = plt.subplots(1,2,figsize=(15,4))\n",
    "    \n",
    "    # Plot loss\n",
    "    loss_plot = axs[0]\n",
    "    \n",
    "    loss_plot.plot(epochs, loss, 'c--', label='Training loss')\n",
    "    loss_plot.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    loss_plot.set_title('Training and validation loss')\n",
    "    loss_plot.set_xlabel('Epochs')\n",
    "    loss_plot.set_ylabel('Loss')\n",
    "    loss_plot.legend()\n",
    "    \n",
    "    # Plot accuracy\n",
    "    acc_plot = axs[1]\n",
    "    \n",
    "    acc_plot.plot(epochs, acc, 'c--', label='Training acc')\n",
    "    acc_plot.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    acc_plot.set_title('Training and validation accuracy')\n",
    "    acc_plot.set_xlabel('Epochs')\n",
    "    acc_plot.set_ylabel('Accuracy')\n",
    "    acc_plot.legend()\n",
    "    \n",
    "def predict_class(model, image_file):\n",
    "    test_image = image.load_img(image_file, target_size=(100,100))\n",
    "    test_image = image.img_to_array(test_image)\n",
    "    test_image /= 255.0\n",
    "    test_image = np.expand_dims(test_image, axis=0)\n",
    "    images = np.vstack([test_image])\n",
    "    predict = model.predict_generator(images).argmax(axis=1)\n",
    "    plt.imshow(test_image)\n",
    "    if predict == 0:\n",
    "            plt.xlabel('predict: cat')\n",
    "    elif predict == 1:\n",
    "        plt.xlabel('predict: dog')\n",
    "    plt.show()\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf464402",
   "metadata": {},
   "source": [
    "### 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbe4d9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir =  './Cat_Dog_data2/train/'\n",
    "test_dir =  './Cat_Dog_data2/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a377b7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_WIDTH = 64\n",
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_SIZE = (IMAGE_WIDTH, IMAGE_HEIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7220372e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68621255",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_train = len([name for name in os.listdir(train_dir) for name in os.listdir(train_dir+name)])\n",
    "nb_test = len([name for name in os.listdir(test_dir) for name in os.listdir(test_dir+name)])\n",
    "# nb_test = len(os.listdir(test_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65939be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22500\n",
      "2500\n"
     ]
    }
   ],
   "source": [
    "print(nb_train)\n",
    "print(nb_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c73e97",
   "metadata": {},
   "source": [
    "### 2. Data preprocessing and data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c2ee6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape =  (IMAGE_WIDTH, IMAGE_HEIGHT,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "705f88e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18000 images belonging to 2 classes.\n",
      "{'cat': 0, 'dog': 1}\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255.0,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True,\n",
    "    validation_split=0.2)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical', subset='training')\n",
    "\n",
    "print(train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30c4f89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4500 images belonging to 2 classes.\n",
      "{'cat': 0, 'dog': 1}\n"
     ]
    }
   ],
   "source": [
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical', subset='validation')\n",
    "\n",
    "print(validation_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3b85ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set\n",
      "[0 1]\n",
      "[9000 9000]\n",
      "Validation set\n",
      "[0 1]\n",
      "[2250 2250]\n"
     ]
    }
   ],
   "source": [
    "print('Training set')\n",
    "filename, label_count = np.unique(train_generator.classes, return_counts=True)\n",
    "print(filename)\n",
    "print(label_count)\n",
    "\n",
    "print('Validation set')\n",
    "val_filename, val_label_count = np.unique(validation_generator.classes, return_counts=True)\n",
    "print(val_filename)\n",
    "print(val_label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a49c60cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator[0][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18c903b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73272b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2500 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = test_datagen.flow_from_directory(test_dir,\n",
    "                                            target_size=IMAGE_SIZE,\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcc5d14",
   "metadata": {},
   "source": [
    "### 3. Build model (Transfer Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b861f6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import vgg16\n",
    "\n",
    "vgg = vgg16.VGG16(include_top=False, \n",
    "                  weights='imagenet',\n",
    "                  input_shape=(64,64,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21a16581",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in vgg.layers[:15]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "224d8be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Functional)           (None, 2, 2, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               262272    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 14,977,218\n",
      "Trainable params: 7,341,954\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_vgg = models.Sequential()\n",
    "new_vgg.add(vgg)\n",
    "new_vgg.add(layers.Flatten())\n",
    "new_vgg.add(layers.Dense(128,activation='relu'))\n",
    "\n",
    "new_vgg.add(layers.Dropout(0.2,name='dropout_2'))\n",
    "new_vgg.add(layers.Dense(2,activation='softmax'))\n",
    "\n",
    "new_vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8bb97905",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optimizers.SGD(lr=0.001, momentum=0.9)\n",
    "new_vgg.compile(optimizer=opt,\n",
    "           loss = 'categorical_crossentropy',\n",
    "           metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc7ddd5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1125/1125 [==============================] - 91s 81ms/step - loss: 0.4345 - accuracy: 0.7996 - val_loss: 0.3555 - val_accuracy: 0.8325\n",
      "Epoch 2/25\n",
      "1125/1125 [==============================] - 92s 82ms/step - loss: 0.3407 - accuracy: 0.8470 - val_loss: 0.3237 - val_accuracy: 0.8577\n",
      "Epoch 3/25\n",
      "1125/1125 [==============================] - 96s 85ms/step - loss: 0.3106 - accuracy: 0.8650 - val_loss: 0.3218 - val_accuracy: 0.8528\n",
      "Epoch 4/25\n",
      "1125/1125 [==============================] - 184s 164ms/step - loss: 0.2785 - accuracy: 0.8787 - val_loss: 0.3023 - val_accuracy: 0.8694\n",
      "Epoch 5/25\n",
      "1125/1125 [==============================] - 132s 118ms/step - loss: 0.2629 - accuracy: 0.8832 - val_loss: 0.3176 - val_accuracy: 0.8625\n",
      "Epoch 6/25\n",
      "1125/1125 [==============================] - 115s 102ms/step - loss: 0.2483 - accuracy: 0.8921 - val_loss: 0.2923 - val_accuracy: 0.8721\n",
      "Epoch 7/25\n",
      "1125/1125 [==============================] - 122s 108ms/step - loss: 0.2283 - accuracy: 0.9006 - val_loss: 0.3026 - val_accuracy: 0.8737\n",
      "Epoch 8/25\n",
      "1125/1125 [==============================] - 128s 113ms/step - loss: 0.2106 - accuracy: 0.9136 - val_loss: 0.3142 - val_accuracy: 0.8639\n",
      "Epoch 9/25\n",
      "1125/1125 [==============================] - 131s 116ms/step - loss: 0.1983 - accuracy: 0.9175 - val_loss: 0.2860 - val_accuracy: 0.8763\n",
      "Epoch 10/25\n",
      "1125/1125 [==============================] - 121s 108ms/step - loss: 0.1837 - accuracy: 0.9265 - val_loss: 0.3222 - val_accuracy: 0.8719\n",
      "Epoch 11/25\n",
      "1125/1125 [==============================] - 108s 96ms/step - loss: 0.1707 - accuracy: 0.9287 - val_loss: 0.2965 - val_accuracy: 0.8808\n",
      "Epoch 12/25\n",
      "1125/1125 [==============================] - 111s 99ms/step - loss: 0.1494 - accuracy: 0.9395 - val_loss: 0.3106 - val_accuracy: 0.8837\n",
      "Epoch 13/25\n",
      "1125/1125 [==============================] - 164s 145ms/step - loss: 0.1380 - accuracy: 0.9436 - val_loss: 0.3707 - val_accuracy: 0.8572\n",
      "Epoch 14/25\n",
      "1125/1125 [==============================] - 120s 107ms/step - loss: 0.1348 - accuracy: 0.9495 - val_loss: 0.3479 - val_accuracy: 0.8717\n",
      "Epoch 15/25\n",
      "1125/1125 [==============================] - 121s 107ms/step - loss: 0.1218 - accuracy: 0.9526 - val_loss: 0.3520 - val_accuracy: 0.8717\n",
      "Epoch 16/25\n",
      "1125/1125 [==============================] - 125s 111ms/step - loss: 0.1145 - accuracy: 0.9537 - val_loss: 0.3624 - val_accuracy: 0.8681\n",
      "Epoch 17/25\n",
      " 377/1125 [=========>....................] - ETA: 1:08 - loss: 0.0991 - accuracy: 0.9616"
     ]
    }
   ],
   "source": [
    "history = new_vgg.fit_generator(train_generator,\n",
    "                           epochs=25,               \n",
    "                           steps_per_epoch = train_generator.samples // BATCH_SIZE,\n",
    "                           validation_data=validation_generator,\n",
    "                           validation_steps = validation_generator.samples // BATCH_SIZE,\n",
    "                           verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efe2c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_result(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c51d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.reset\n",
    "ytesthat = new_model.predict_generator(test_set)\n",
    "df = pd.DataFrame({\n",
    "    'filename':test_set.filenames,\n",
    "    'predict':ytesthat[:,0],\n",
    "    'y':test_set.classes\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF2.4",
   "language": "python",
   "name": "tf2.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
